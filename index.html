<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

	<title>Puja Trivedi</title>
	
	<meta name="author" content="Puja Trivedi">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
	<link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="images/png" href="images/umich_seal.png">
</head>

<body>
	<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		<tr style="padding:0px">
			<td style="padding:0px">
				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr style="padding:0px">
						<td style="padding:2.5%;width:63%;vertical-align:middle">
							<p style="text-align:center">
								<name>Puja Trivedi</name>
							</p>

				<!-- Description -->
						<p> I am a CSE PhD candidate in the Graph Exploration and Mining at Scale <a href="https://gemslab.github.io/">(GEMS)</a> Lab at the University of Michigan, where I am fortunate to be advised by <a href="http://web.eecs.umich.edu/~dkoutra/">Prof. Danai Koutra</a>. I also often often collaborate with by <a href="https://jjthiagarajan.com/">Dr. Jay Thiagarajan</a> at Lawrence Livermore National Laboratory. 
						</p>

						<p>
						I am broadly interested in how self-supervised learning can be performed effectively and reliably for non-euclidean and graph data by incorporating domain invariances and designing grounded algorithms. My recent work has focused on understanding the role of data augmentations in graph contrastive learning. 
						</p>

				<!-- Contact -->
							<p style="text-align:center">
								<a href="mailto:pujat@umich.edu">Email</a> &nbsp/&nbsp
								<a href="PujaTrivedi_cv.pdf">CV</a> &nbsp/&nbsp
								<a href="https://scholar.google.com/citations?user=1y9cR50AAAAJ&hl=en"> Google Scholar</a>
							</p>
			</td>
			
			<!-- Profile Image -->
						<td style="padding:2.5%;width:40%;max-width:40%">
							<a href="images/PujaTrivedi.JPEG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/PujaTrivedi_circle.png" class="hoverZoomLink"></a>
						</td>
					</tr>
				</tbody></table>

			<!-- News -->
			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr> <td width="100%" valign="top">
			<heading>News</heading>
			<table width="100%" valign="top" border="0" cellspacing="0" cellpadding="2">
			
			<tr valign="top"> <td> [04/2022] </td>  <td> 
			Our work proposing a <a href="https://graph-learning-benchmarks.github.io/assets/papers/glb2022/A_Content_First_Benchmark_for_Self_Supervised_Graph_Representation_Learning.pdf"> synthetic benchmark for graph contrastive learning </a> was accepted at the Workshop on Graph Learning Benchmarks at TheWebConf (formerly WWW), 2022. <strong>Stay tuned for our follow-up with generalization bounds for GCL!</strong>  
			
			</td> </tr>
			<tr valign="top"> <td> [01/2022] </td>  <td> 
			Our work on <a href="https://arxiv.org/abs/2111.03220">unsupervised graph representation learning</a> was accepted to TheWebConf (formerly WWW), 2022.  
			</td> </tr>
				
			</td> </tr>
			<tr valign="top"> <td> [09/2021] </td>  <td> 
			Our work on  <a href="https://arxiv.org/pdf/2111.05410.pdf">understanding neural network dynamics through a dynamic graph perspective</a> was accepted as a contributed talk at the Women in Machine Learning (WiML) workshop at NeurIPS, 2021. 
			</td> </tr>
			
			</td> </tr>
			<tr valign="top"> <td> [07/2021] </td>  <td> 
			Our work on <a href="">understanding how quadractic regularizers mitigate catastrophic forgetting </a> was accepted at the Theory and Foundation of Continual Learning Workshop at ICML, 2021.
			</td> </tr>
			
			</td> </tr>
			<tr valign="top"> <td> [05/2021] </td>  <td> 
			Started interning with <a href="https://jjthiagarajan.com/">Dr. Jay Thiagarajan</a> at Lawrence Livermore National Laboratory. 
			</td> </tr>

			</td> </tr>
			<tr valign="top"> <td> [09/2020] </td>  <td> 
			Our work on <a href="">understanding structural and proximity-based temporal embeddings</a> was accepted at the Mining and Learning with Graphs workshop at KDD, 2020. 
			</td> </tr>
			
			
			<tr valign="top"> <td> [08/2019] </td>  <td> 
			Started Computer science and Engineering PhD in the Graph Exploration and Mining at Scale <a href="https://gemslab.github.io/">(GEMS)</a> Lab at the University of Michigan. 
			</td> </tr>
			
			<tr valign="top"> <td> [07/19] </td>  <td> 
			Completed a summer internship at <a href="https://sift.net">SIFT</a> in Minneapolis, MN. Worked on Bayesian Experimental Modeling with <a href="http://rpgoldman.real-time.com">Dr. Robert Goldman</a>.	
			</td> </tr>
			
			<tr valign="top"> <td> [05/2019] </td>  <td> 
			Graduated from UMBC with dual degrees in Mathematics and Computer Science. 
			</td> </tr>
			</table>
			</td>  </tr> </tbody></table>	
		  <!-- Publications -->
			<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				<tr>
				<td style="padding:20px;width:100%;vertical-align:middle">
					<heading>Publications</heading>
				</td>
			</tr>
		</tbody></table>
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
					<img src="images/svc_3.png" alt="www" width="160" style="border-style: none">
				</td>
				<td width="75%" valign="middle">
					<a href="https://graph-learning-benchmarks.github.io/assets/papers/glb2022/A_Content_First_Benchmark_for_Self_Supervised_Graph_Representation_Learning.pdf">
						<papertitle>A Content-First Benchmark for Self-Supervised Graph
							Representation Learning</papertitle>
					</a>
					<br>
					<strong>Puja Trivedi</strong>,
					<a href="https://markheimann.github.io/"> Mark Heimann</a>,
					<a href="https://ekdeepslubana.github.io/"> Ekdeep Singh Lubana</a>,
					<a href="https://web.eecs.umich.edu/~dkoutra/">Danai Koutra</a>, and
					<a href="https://jjthiagarajan.com/">Jayaraman J. Thiagarajan</a>
					<br>
					<em>TheWebConf Workshop on Graph Learning Benchmarks</em>, 2022
					<br>
					<a href="assets/glb.bib">bibtex</a> / <a href="https://graph-learning-benchmarks.github.io/assets/papers/glb2022/A_Content_First_Benchmark_for_Self_Supervised_Graph_Representation_Learning.pdf">PDF</a> / <a href="assets/14-A Content-First Benchmark for Self-Supervised Graph Representation Learning.mp4"> Video</a> / <a href="https://drive.google.com/drive/folders/170TlKMask7pFAvN8AeEwLA7yUU1EhUle?usp=sharing"> Data</a>
					<p>We introduce a synthetic data generation process that allows us to control the amount of task-irrelevant and task-relevant information in graph datasets. We find that it is particularly useful in evaluating automated augmentations methods. </p>
				</td>
			</tr>			

			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
					<img src="images/webconf.png" alt="www" width="160" style="border-style: none">
				</td>
				<td width="75%" valign="middle">
					<a href="https://arxiv.org/pdf/2111.03220.pdf">
						<papertitle>Augmentations in Graph Contrastive Learning: Current Methodological Flaws & Towards Better Practices</papertitle>
					</a>
					<br>
					<strong>Puja Trivedi</strong>,
					<a href="https://ekdeepslubana.github.io/"> Ekdeep Singh Lubana</a>
					<a href="https://sites.google.com/umich.edu/yujunyan/home">Yujun Yan</a>,
					<a href="https://sites.google.com/site/yangyaoqingcmu/">Yaoqing Yang</a>, and
					<a href="https://web.eecs.umich.edu/~dkoutra/">Danai Koutra</a>
					<br>
					<em>ACM The Web Conference (formerly WWW)</em>, 2022
					<br>
					<a href="assets/">bibtex</a> / <a href="https://arxiv.org/abs/2111.03220">arXiv</a> / <a href="assets/webconf_talk.mp4"> Video</a>
					<p>We contextualize the performance of several unsupervised graph representation learning methods with respect to inductive bias of GNNs and show significant improvements by using structured augmentations defined by task-relevance.</p>
				</td>
			</tr>			


			<tr>
				<td style="padding:20px;width:25%;vertical-align:middle">
				<img src="images/emr.png" alt="gradflow" width="160" style="border-style: none">
				</td>
				<td width="75%" valign="middle">
				<a href="https://arxiv.org/pdf/2102.02805.pdf">
					<papertitle>How do Quadratic Regularizers Prevent Catastrophic Forgetting: The Role of Interpolation</papertitle>
				</a>
				<br>
				<a href="https://ekdeepslubana.github.io/"> Ekdeep Singh Lubana</a>,
				<strong> Puja Trivedi</strong>,
				<a href="https://web.eecs.umich.edu/~dkoutra/">Danai Koutra</a>, and
				<a href="http://robertdick.org/">Robert P. Dick</a>
				<br>
				<em>ICML Workshop on Theory and Foundations of Continual Learning</em>, 2021 
				<br>
				<a href="assets/emr.bib">bibtex</a> / <a href="https://github.com/EkdeepSLubana/QRforgetting">github</a> / <a href="https://arxiv.org/abs/2102.02805">arXiv</a>
				<p>This work demonstrates how quadratic regularization methods for preventing catastrophic forgetting in deep networks rely on a simple heuristic under-the-hood: Interpolation.</p>
				</td>
			</tr>




		</table>
		</td>  </tr> </tbody></table>

				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
						<td style="padding:0px">
							<br>
							<p style="text-align:right;font-size:small;">
								Website Design from: <a href="https://github.com/jonbarron/jonbarron_website">here.</a>
							</p>
						</td>
					</tr>
				</tbody></table>
			</td>
		</tr>
	</table>
</body>

</html>
